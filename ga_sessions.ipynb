{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "\n",
    "In this final project you will have the opportunity to apply everything you've learned throughout the course on a similar setting to what you face on your daily work as a Data Analyst at Mercedes.\n",
    "\n",
    "The goal of this project is to understand how you can manipulate and analyze Google Analytics data about user interactions on a website.\n",
    "\n",
    "For that, you'll work with the [Google Analytics Sample dataset](https://console.cloud.google.com/marketplace/product/obfuscated-ga360-data/obfuscated-ga360-data?inv=1&invt=AbmlmQ), which contains real data from the [Google Merchandise Store](https://shop.googlemerchandisestore.com/), a real ecommerce store that sells Google-branded merchandise.\n",
    "\n",
    "The data is typical of what an ecommerce website would see and includes the following information:\n",
    "- **Traffic source data**: information about where website visitors originate, including data about organic traffic, paid search traffic, and display traffic\n",
    "- **Content data**: information about the behavior of users on the site, such as URLs of pages that visitors look at, how they interact with content, etc.\n",
    "- **Transactional data**: information about the transactions on the Google Merchandise Store website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "The data is available on a zip file. This zip contains three parquet files:\n",
    "- `ga_sessions_main.parquet`: the main information about each session\n",
    "- `ga_sessions_hits.parquet`: detailed information about hits in each session\n",
    "- `ga_sessions_network.parquet`: information about traffic sources, device and geographic information\n",
    "\n",
    "**NOTE:** To make things a bit easier, only data from the first 15 days of August 2016 was included in the dataset. Also, some noisy information about `hits` was removed from the original data.\n",
    "\n",
    "Let's download the data and save it to the Databricks File System (DBFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh wget https://raw.githubusercontent.com/inesmcm26/lp-big-data-mercedes/main/data/ga_sessions.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh unzip ga_sessions.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.cp('file:/databricks/driver/ga_sessions_main.parquet', 'dbfs:/FileStore/final_project/ga_sessions_main.parquet')\n",
    "dbutils.fs.cp('file:/databricks/driver/ga_sessions_network.parquet', 'dbfs:/FileStore/final_project/ga_sessions_network.parquet')\n",
    "dbutils.fs.cp('file:/databricks/driver/ga_sessions_hits.parquet', 'dbfs:/FileStore/final_project/ga_sessions_hits.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column that identifies a session and is **common to all tables** is the `sessionId` column.\n",
    "\n",
    "Run the following cell to load each dataset into spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = spark.read.parquet('/FileStore/final_project/ga_sessions_main.parquet')\n",
    "df_hits = spark.read.parquet('/FileStore/final_project/ga_sessions_hits.parquet')\n",
    "df_network = spark.read.parquet('/FileStore/final_project/ga_sessions_network.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the session id, the main dataset contains the following columns:\n",
    "- **visitorId**: The unique identifier for a visitor\n",
    "- **visitNumber**: The visit number of this user. If this is the first visit to the website, then this is set to 1.\n",
    "- **visitStartTime**: The timestamp (expressed as POSIX time) of the beginning of the session\n",
    "- **totals**: A struct with statistics about the session, such as total number of hits, time on site, number of transactions and revenue, etc.\n",
    "- **channelGrouping**: The channel via which the user came to the Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hits.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the session id, the hits dataset contains the following columns:\n",
    "\n",
    "- **hits**: An array of structs representing all the hits in this session. A hit is an interaction that results in data being sent to Google Analytics. Each struct is a hit defined by the following fields:\n",
    "    - **hitNumber**: The number of this hit in the session\n",
    "    - **type**: Type of the hit (PAGE or EVENT)\n",
    "    - **hour**: Hour of the hit\n",
    "    - **minute**: Minute of the hit\n",
    "    - **time**: Time spent on the hit\n",
    "    - **page**: Information about the page\n",
    "    - **contentGroup**: Information about the content categorization of the page on the website\n",
    "    - **product**: Array of structs with product information of all products displayed on the page\n",
    "    - **eventInfo**: If hit is of type 'EVENT', this field contains information about the event\n",
    "    - **promotion**: Array of structs with promotion information of all promotions displayed on the page.\n",
    "    - **promotionActionInfo**: Present when there is a promotion on the hit. It explains whether the promotion was clicked (which corresponds to a hit of type 'EVENT' and this event is a 'Promotion Click'), or the promotion is just viewed on the page but was not clicked. \n",
    "    - **transaction**: Information about the transaction when the hit is an event 'Confirm Checkout'. Null otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the session id, the network dataset contains the following columns:\n",
    "\n",
    "- **trafficSource**: A struct with information about the source of the session, as well as adds and campaign information\n",
    "- **device**: A struct with information about the device used in the session\n",
    "- **geoNetwork**: A struct with information about the geographic location of the user. Most of this information is obscured and only city, country and country are available.\n",
    "- **customDimensions**: Extra traffic information. You can ignore this column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by checking how many rows the dataframes have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main.count())\n",
    "print(df_hits.count())\n",
    "print(df_network.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr_rows = WRITE THE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(nr_rows, int)\n",
    "    assert hashlib.sha256(str(nr_rows).encode('utf-8')).hexdigest() == '5a0d24b5fcc0584bfd8d51a4fa0ebb838ef1e9769707e17c7e8002438909e383'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see if there are any missing values on the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that if the channel grouping is missing, the channel via which the user came to the Store is 'Direct' and fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = (\n",
    "    df_main\n",
    "    .fillna('Direct', subset=['channelGrouping'])\n",
    ")\n",
    "\n",
    "df_main.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "channel_grouping_values = df_main.select('channelGrouping').rdd.flatMap(lambda x: x).collect()\n",
    "try:\n",
    "    assert None not in channel_grouping_values\n",
    "    assert channel_grouping_values.count('Direct') == 9587\n",
    "    print('Good job! You managed to fill the missing values values')\n",
    "except:\n",
    "    print('Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a closer look into the `geoNetwork` column of the network dataframe. Count how many times a session is derived from each continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_network\n",
    "    .select(\n",
    "        f.col('geoNetwork').getField('continent').alias('continent')\n",
    "    )\n",
    "    .groupBy('continent')\n",
    "    .agg(f.count('continent').alias('count'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the problem? Sometimes a continent is written in lower case and other in upper case. Standardize the data to make the continent always lower case.\n",
    "\n",
    "**Hint:** You learned how to lower case a String column during the course. But now we want to lower case the field `continent` of the struct column `geoNetwork`. [Here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html) you can check PySpark column methods. One of them will help you solving the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = (\n",
    "    df_network\n",
    "    .withColumn(\n",
    "        'geoNetwork',\n",
    "        f.col('geoNetwork').withField('continent', f.lower(f.col('geoNetwork').getField('continent')))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "continents = df_network.select(f.col('geoNetwork').getField('continent').alias('continent')).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "try:\n",
    "    for continent in continents:\n",
    "        assert continent == continent.lower()\n",
    "    print('Good job! You managed to standardize the data')\n",
    "except:\n",
    "    print('Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer business questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Users access the store through different channels, and each session has a corresponding revenue value.\n",
    "\n",
    "1. Which channel generates the highest total revenue across all sessions?\n",
    "\n",
    "Notes:\n",
    "- Use the `channelGrouping` column in the main dataset for channel types.\n",
    "- Calculate the revenue using the `totalTransactionRevenue` field within the `totals` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_main\n",
    "    .groupBy('channelGrouping')\n",
    "    .agg(\n",
    "        f.sum(f.col('totals').getField('totalTransactionRevenue')).alias('revenue')\n",
    "    )\n",
    "    .orderBy(f.desc('revenue'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel = WRITE THE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = \"Referral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(channel, str)\n",
    "    assert hashlib.sha256(channel.encode('utf-8')).hexdigest() == 'aeb7b00433003f75c286e214eccd11a1e4ba6fbd0d0413cb35864749821cc8e0'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Users access the store through different browsers. Which are the top 3 browsers ranked by the total time users spent on the site?\n",
    "\n",
    "Notes:\n",
    "- You can find the browser used by a user on a session in the `device` column of the network dataframe\n",
    "- The total time spent on site on a session is registered on the `totals` column of the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_main\n",
    "    .join(\n",
    "        df_network,\n",
    "        on=['sessionId']\n",
    "    )\n",
    "    .select(\n",
    "        f.col('device').getField('browser').alias('browser'),\n",
    "        f.col('totals').getField('timeOnSite').alias('time')\n",
    "    )\n",
    "    .groupBy('browser')\n",
    "    .agg(f.sum(f.col('time')).alias('total_time'))\n",
    "    .orderBy(f.desc('total_time'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_browsers = ['Chrome', 'Safari', 'Firefox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_browsers = ['Browser1', 'Browser2', 'Broswer3'] REPLACE THE VALUES WITH THE CORRECT ONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(top_browsers, list)\n",
    "    assert len(top_browsers) == 3\n",
    "    for browser in top_browsers:\n",
    "        assert isinstance(browser, str)\n",
    "    assert hashlib.sha256(json.dumps(''.join(top_browsers)).encode()).hexdigest() == '13a1c6bec3d2d1c3d8be28dbf835880972effd2ff9619f1b15d6fee8d505ff5c'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')\n",
    "    print('Check if you wrote the browser names starting with capital letter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analyse the website traffic (total number of sessions) per hour of the day and day of the week.\n",
    "\n",
    "Visualize the result using a pivot table.\n",
    "\n",
    "**NOTE:** The start time of each session is in UNIX time. You may have to first transform it to a date before being able to extract the hour and day of week.\n",
    "\n",
    "What is the total number of sessions registered at 8pm on tuesdays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_main\n",
    "    .withColumn('date', f.from_unixtime('visitStartTime'))\n",
    "    .withColumn('day_of_week', f.dayofweek('date'))\n",
    "    .withColumn('hour', f.hour('date'))\n",
    "    .groupBy('hour')\n",
    "    .pivot('day_of_week')\n",
    "    .agg(\n",
    "        f.count('sessionId')\n",
    "    )\n",
    "    .orderBy('hour')\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr_sessions = WRITE THE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_sessions = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(nr_sessions, int)\n",
    "    assert hashlib.sha256(str(nr_sessions).encode('utf-8')).hexdigest() == '5426d2ca50f244fb43fe9eafc82da08f33f3b4f8d9140802bd0102e780b629d6'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Identify the `visitorId` of the user with highest average time gap between two consecutive sessions. Consider only visitors that have more than 6 registered sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('visitorId').orderBy('visitStartTime')\n",
    "\n",
    "(\n",
    "    df_main\n",
    "    .withColumn('date', f.from_unixtime('visitStartTime'))\n",
    "    .withColumn('prev_date', f.lag('date').over(window))\n",
    "    .withColumn('days_gap', f.date_diff('date', 'prev_date'))\n",
    "    .groupBy('visitorId')\n",
    "    .agg(\n",
    "        f.count('sessionId').alias('nr_sessions'),\n",
    "        f.avg('days_gap').alias('avg_days_gap')\n",
    "    )\n",
    "    .filter(f.col('nr_sessions') > 6)\n",
    "    .orderBy(f.desc('avg_days_gap'))\n",
    "    .limit(1)\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visitor_id = WRITE THE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_id = '8436426603099391262'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(visitor_id, str)\n",
    "    assert hashlib.sha256(visitor_id.encode('utf-8')).hexdigest() == '5abdd1c91bc52becf7267a3bacfb9d5e979f54f234919d982ed0c1f61cb6425a'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are the top 5 products that are most added to the cart?\n",
    "\n",
    "**NOTES:**\n",
    "- A hit of type 'EVENT' can correspond to one of the following event actions (`eventAction` field of `eventInfo`):\n",
    "    - Product Click\n",
    "    - Add to Cart\n",
    "    - Remove from Cart\n",
    "    - Quickview Click\n",
    "    - Onsite Click\n",
    "    - Promotion Click\n",
    "- A product is identified by its SKU value. You can find this value in field `productSKU` of a product. Remember that the `product` field is an array of product information of all products involved in a hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_hits\n",
    "    .select(\n",
    "        'sessionId',\n",
    "        f.inline('hits')\n",
    "    )\n",
    "    .filter(\n",
    "        (f.col('type') == 'EVENT')\n",
    "        & (f.col('eventInfo').getField('eventAction') == \"Add to Cart\")\n",
    "    )\n",
    "    .groupBy(\n",
    "        f.element_at(f.col('product'), 1).getField('productSKU').alias('product_sku'),\n",
    "        f.element_at(f.col('product'), 1).getField('v2ProductName').alias('product_name')\n",
    "    )\n",
    "    .agg(f.count('sessionId').alias('nr_added_to_cart'))\n",
    "    .orderBy(f.desc('nr_added_to_cart'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_producs = [\"product_sku_1\", \"product_sku_2\", ...] REPLACE THE VALUES WITH THE CORRECT ONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products = [\"GGOEGFKQ020399\", \"GGOEGAAX0037\", \"GGOEGAAX0104\", \"GGOEGAAX0342\", \"GGOEGAAX0074\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(top_products, list)\n",
    "    assert len(top_products) == 5\n",
    "    for browser in top_products:\n",
    "        assert isinstance(browser, str)\n",
    "    assert hashlib.sha256(json.dumps(''.join(top_products)).encode()).hexdigest() == '2758b9ec187f6a3f3d13a35a194c82b92e12ba7b995037a84c3f4262dacac35f'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')\n",
    "    print('Make sure you wrote the product ids in the correct order')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the average time spent by users on the 'Shopping Cart' page in sessions where a purchase was made?\n",
    "\n",
    "Answer with 2 decimal places.\n",
    "\n",
    "**NOTES**\n",
    "- To determine sessions where purchases were made, filter the main dataframe by checking the `transactions` field of the `totals` column. If the field is non-null and greater than 0, it indicates that a purchase occurred during the session.\n",
    "- Hits that correspond to users being on the 'Shopping Cart' page are of type 'PAGE', and the `pageTitle` field in `page` is 'Shopping Cart'.\n",
    "- The time spent on a hit is available on the `time` field of the `hits` column of the hits dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_hits\n",
    "    .select(\n",
    "        'sessionId',\n",
    "        f.inline('hits')\n",
    "    )\n",
    "    .join(\n",
    "        df_main,\n",
    "        on=['sessionId']\n",
    "    )\n",
    "    .filter(\n",
    "        f.col('totals').getField('transactions').isNotNull()\n",
    "        & (f.col('totals').getField('transactions') > 0)\n",
    "        & (f.col('type') == 'PAGE')\n",
    "        & (f.col('page').getField('pageTitle') == 'Shopping Cart')\n",
    "    )\n",
    "    .select(f.round(f.avg('time'), 2).alias('avg_time'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_time = WRITE THE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time = 767495.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(avg_time, float)\n",
    "    assert hashlib.sha256(str(avg_time).encode('utf-8')).hexdigest() == '5434c334d8669be826679ae262c9feb5b895a269d1afde5fd2bdb3fe24fa2d2c'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Considering only sessions where there was a promotion click and at least one product was added to the cart, what is the id of the most clicked promotion?\n",
    "\n",
    "**NOTES:**\n",
    "- You can check if a product was added to the cart or a promotion was clicked by analysing the `eventInfo` column. A hit of type 'EVENT' can correspond to one of the following event actions (`eventAction` field of `eventInfo` column):\n",
    "    - Product Click\n",
    "    - Add to Cart\n",
    "    - Remove from Cart\n",
    "    - Quickview Click\n",
    "    - Onsite Click\n",
    "    - Promotion Click\n",
    "- For hits where there was a promotion click, the column `promotion` contains an array with only element - details of the clicked promotion. You can find the promotion id on the field `promoId` of the element in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_hits\n",
    "    .withColumn(\n",
    "        'events_list',\n",
    "        f.transform(\n",
    "            'hits',\n",
    "            lambda hit: f.when(hit.getField('type') == 'EVENT', hit.getField('eventInfo').getField('eventAction')).otherwise('None')\n",
    "        )\n",
    "    )\n",
    "    .filter(\n",
    "        f.array_contains('events_list', 'Add to Cart')\n",
    "        & f.array_contains('events_list', 'Promotion Click')\n",
    "    )\n",
    "    .select(\n",
    "        'visitId',\n",
    "        f.inline('hits')\n",
    "    )\n",
    "    .groupBy(f.element_at('promotion', 1).getField('promoId'))\n",
    "    .agg(f.count('visitId').alias('nr_clicks'))\n",
    "    .orderBy(f.desc('nr_clicks'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo_id = WRITE THE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_id = \"Apparel Row 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(promo_id, str)\n",
    "    assert hashlib.sha256(promo_id.encode('utf-8')).hexdigest() == '2f34cf7b9ce1f5062f0fe6f8f9a6d073214fc6869ba4a85014bab1cf672e80cc'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Identify the user that most views promotions in sessions but never clicks on them.\n",
    "\n",
    "Use a UDF to answer the question.\n",
    "\n",
    "**NOTES:**\n",
    "- You can check if promotions where viewed on a hit by checking the `promoIsView` field of the `promotionActionInfo` column.\n",
    "- Similarly, you can see if a user clicked on a promotion on a hit by checking the `promoIsClick` field of the `promotionActionInfo` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "def view_but_no_click(hits_list):\n",
    "    clicked = False\n",
    "    view = False\n",
    "    for hit in hits_list:\n",
    "        if hit['promotionActionInfo']:\n",
    "            if hit['promotionActionInfo']['promoIsClick']:\n",
    "                clicked = True\n",
    "            if hit['promotionActionInfo']['promoIsView']:\n",
    "                view = True\n",
    "    \n",
    "    return view and not clicked\n",
    "\n",
    "view_but_no_click_udf = f.udf(view_but_no_click, BooleanType())\n",
    "\n",
    "(\n",
    "    df_hits\n",
    "    .join(\n",
    "        df_main,\n",
    "        on=['sessionId']\n",
    "    )\n",
    "    .withColumn(\n",
    "        'view_but_no_click',\n",
    "        view_but_no_click_udf(f.col('hits'))\n",
    "    )\n",
    "    .filter(f.col('view_but_no_click'))\n",
    "    .groupBy('visitorId')\n",
    "    .agg(f.sum(f.when(f.col('view_but_no_click'), 1).otherwise(0)).alias('nr_sessions'))\n",
    "    .orderBy(f.desc('nr_sessions'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visitor_id = WRITE THE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_id = '0593150394512575588'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test to verify that the answer is correct\n",
    "try:\n",
    "    assert isinstance(visitor_id, str)\n",
    "    assert hashlib.sha256(visitor_id.encode('utf-8')).hexdigest() == '230dd9fdc397961e79b1a698614b91f948ac7ab685261b7d21bf842387768fe9'\n",
    "    print('Good job! The answer is correct')\n",
    "except:\n",
    "    print('The answer is not right yet! Try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions path analysis - Simple analysis\n",
    "\n",
    "We'll focus on these pages to understand where users are most lost in the process:\n",
    "\n",
    "| Page | pagePathLevel1 |\n",
    "| - | - |\n",
    "| Home | /home |\n",
    "| Item | /google+redesign/ |\n",
    "| Shopping Cart | /basket.html|\n",
    "| Payment Method | /payment.html |\n",
    "| Checkout Confirmation | /ordercompleted.html |\n",
    "\n",
    "We want to calculate the number and percentage of sessions that stop at each of these stages.\n",
    "\n",
    "The sequence we're monitoring is the same as the one presented above:\n",
    "\n",
    "Home -> Item -> Shopping Cart -> Payment Method -> Checkout Confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The way you currently do it\n",
    "\n",
    "To understand how many sessions stop at each stage, you create one table containing the hits for each page and do multiple left joins to monitor the sequence of hits.\n",
    "\n",
    "First of all, lets process the hits table to put it on the format you're used to dealing: row per hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_hits = (\n",
    "    df_hits\n",
    "    .select(\n",
    "        'sessionId',\n",
    "        f.inline('hits')\n",
    "    )\n",
    "    .select(\n",
    "        'sessionId',\n",
    "        'hitNumber',\n",
    "        f.col('page').getField('pagePathLevel1').alias('pagePath')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_exploded_hits.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create one table with the hits for each page and then do the multiple left joins.\n",
    "\n",
    "This corresponds to the following SQL code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register PySpark DataFrame as SQL temporary view to manipulate it with SQL sintax\n",
    "df_exploded_hits.createOrReplaceTempView('hits_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "WITH filtered_hits AS (\n",
    "  SELECT sessionId, pagePath, hitNumber\n",
    "  FROM hits_view\n",
    "  WHERE pagePath IN (\"/home\", \"/google+redesign/\", \"/basket.html\", \"/yourinfo.html\", \"/payment.html\", \"/revieworder.html\", \"/ordercompleted.html\")\n",
    "),\n",
    "home_hits AS (\n",
    "  SELECT sessionId, hitNumber\n",
    "  FROM filtered_hits\n",
    "  WHERE pagePath == \"/home\"\n",
    "),\n",
    "item_hits AS (\n",
    "  SELECT sessionId, hitNumber\n",
    "  FROM filtered_hits\n",
    "  WHERE pagePath == \"/google+redesign/\"\n",
    "),\n",
    "basket_hits AS (\n",
    "  SELECT sessionId, hitNumber\n",
    "  FROM filtered_hits\n",
    "  WHERE pagePath == \"/basket.html\"\n",
    "),\n",
    "payment_hits AS (\n",
    "  SELECT sessionId, hitNumber\n",
    "  FROM filtered_hits\n",
    "  WHERE pagePath == \"/payment.html\"\n",
    "),\n",
    "order_completed_hits AS (\n",
    "  SELECT sessionId, hitNumber\n",
    "  FROM filtered_hits\n",
    "  WHERE pagePath == \"/ordercompleted.html\"\n",
    "),\n",
    "uni AS (\n",
    "  SELECT\n",
    "    home_hits.sessionId AS home_sessionId, \n",
    "    item_hits.sessionId AS item_sessionId,\n",
    "    basket_hits.sessionId AS basket_sessionId,\n",
    "    payment_hits.sessionId AS payment_sessionId,\n",
    "    order_completed_hits.sessionId AS order_completed_sessionId\n",
    "  FROM home_hits\n",
    "  LEFT JOIN item_hits ON home_hits.sessionId == item_hits.sessionId AND home_hits.hitNumber < item_hits.hitNumber\n",
    "  LEFT JOIN basket_hits ON home_hits.sessionId == basket_hits.sessionId AND item_hits.hitNumber < basket_hits.hitNumber\n",
    "  LEFT JOIN payment_hits ON home_hits.sessionId == payment_hits.sessionId AND basket_hits.hitNumber < payment_hits.hitNumber\n",
    "  LEFT JOIN order_completed_hits ON home_hits.sessionId == order_completed_hits.sessionId AND payment_hits.hitNumber < order_completed_hits.hitNumber\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  COUNT(DISTINCT home_sessionId) AS total_home,\n",
    "  COUNT(DISTINCT item_sessionId) AS total_item,\n",
    "  (COUNT(DISTINCT item_sessionId) / COUNT(DISTINCT home_sessionId)) * 100 AS 1st_2nd,\n",
    "  COUNT(DISTINCT basket_sessionId) AS basket_item,\n",
    "  (COUNT(DISTINCT basket_sessionId) / COUNT(DISTINCT item_sessionId)) * 100 AS 2nd_3rd,\n",
    "  COUNT(DISTINCT payment_sessionId) AS payment_item,\n",
    "  (COUNT(DISTINCT payment_sessionId) / COUNT(DISTINCT basket_sessionId)) * 100 AS 3th_4th,\n",
    "  COUNT(DISTINCT order_completed_sessionId) AS order_completed_item,\n",
    "  (COUNT(DISTINCT order_completed_sessionId) / COUNT(DISTINCT payment_sessionId)) * 100 AS 4th_5th\n",
    "FROM uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the corresponding code in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hits based on specific page paths\n",
    "filtered_hits = (\n",
    "    df_exploded_hits\n",
    "    .filter(\n",
    "        f.col('pagePath').isin(\n",
    "            '/home',\n",
    "            '/google+redesign/',\n",
    "            '/basket.html',\n",
    "            '/payment.html',\n",
    "            '/ordercompleted.html'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create individual DataFrames for each page path\n",
    "home_hits = (\n",
    "    filtered_hits\n",
    "    .filter(f.col('pagePath') == '/home')\n",
    "    .select(\n",
    "        f.col('sessionId').alias('home_sessionId'),\n",
    "        f.col('hitNumber').alias('home_hitNumber')\n",
    "    )\n",
    ")\n",
    "item_hits = (\n",
    "    filtered_hits\n",
    "    .filter(f.col('pagePath') == '/google+redesign/')\n",
    "    .select(\n",
    "        f.col('sessionId').alias('item_sessionId'),\n",
    "        f.col('hitNumber').alias('item_hitNumber')\n",
    "    )\n",
    ")\n",
    "basket_hits = (\n",
    "    filtered_hits\n",
    "    .filter(f.col('pagePath') == '/basket.html')\n",
    "    .select(\n",
    "        f.col('sessionId').alias('basket_sessionId'),\n",
    "        f.col('hitNumber').alias('basket_hitNumber')\n",
    "    )\n",
    ")\n",
    "payment_hits = (\n",
    "    filtered_hits\n",
    "    .filter(f.col('pagePath') == '/payment.html')\n",
    "    .select(\n",
    "        f.col('sessionId').alias('payment_sessionId'),\n",
    "        f.col('hitNumber').alias('payment_hitNumber')\n",
    "    )\n",
    ")\n",
    "order_completed_hits = (\n",
    "    filtered_hits\n",
    "    .filter(f.col('pagePath') == '/ordercompleted.html')\n",
    "    .select(\n",
    "        f.col('sessionId').alias('order_completed_sessionId'),\n",
    "        f.col('hitNumber').alias('order_completed_hitNumber')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Perform left joins in sequence\n",
    "uni = (\n",
    "    home_hits\n",
    "    .join(\n",
    "        item_hits,\n",
    "        on=[(home_hits.home_sessionId == item_hits.item_sessionId)\n",
    "            & (home_hits.home_hitNumber < item_hits.item_hitNumber)\n",
    "        ],\n",
    "        how='left'\n",
    "    )\n",
    "    .join(\n",
    "        basket_hits,\n",
    "        on=[(home_hits.home_sessionId == basket_hits.basket_sessionId)\n",
    "            & (item_hits.item_hitNumber < basket_hits.basket_hitNumber)\n",
    "        ],\n",
    "        how='left'\n",
    "    )\n",
    "    .join(\n",
    "        payment_hits,\n",
    "        on=[(home_hits.home_sessionId == payment_hits.payment_sessionId)\n",
    "            & (basket_hits.basket_hitNumber < payment_hits.payment_hitNumber)\n",
    "        ],\n",
    "        how='left'\n",
    "    )\n",
    "    .join(\n",
    "        order_completed_hits,\n",
    "        on=[(home_hits.home_sessionId == order_completed_hits.order_completed_sessionId)\n",
    "            & (payment_hits.payment_hitNumber < order_completed_hits.order_completed_hitNumber)\n",
    "        ],\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "result = (\n",
    "    uni.select(\n",
    "        f.countDistinct('home_sessionId').alias('total_home'),\n",
    "        f.countDistinct('item_sessionId').alias('total_item'),\n",
    "        f.countDistinct('basket_sessionId').alias('total_basket'),\n",
    "        f.countDistinct('payment_sessionId').alias('total_payment'),\n",
    "        f.countDistinct('order_completed_sessionId').alias('total_order_completed')\n",
    "    )\n",
    "    .withColumn('1st_2nd', (f.col('total_item') / f.col('total_home')) * 100)\n",
    "    .withColumn('2nd_3rd', (f.col('total_basket') / f.col('total_item')) * 100)\n",
    "    .withColumn('3th_4th', (f.col('total_payment') / f.col('total_basket')) * 100)\n",
    "    .withColumn('4th_5th', (f.col('total_order_completed') / f.col('total_payment')) * 100)\n",
    ")\n",
    "\n",
    "result.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The PySpark built-in functions way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it in a more efficient way without the need for multiple left joins.\n",
    "\n",
    "First, start by creating one boolean column for each page path that indicates if the hit is on that page.\n",
    "\n",
    "Complete the code bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pages_flag = (\n",
    "    df_exploded_hits\n",
    "    .withColumn(\"is_home\", (f.col(\"pagePath\") == \"/home\"))\n",
    "    .withColumn(\"is_item\", (f.col(\"pagePath\") == \"/google+redesign/\"))\n",
    "    .withColumn(\"is_basket\", (f.col(\"pagePath\") == \"/basket.html\"))\n",
    "    .withColumn(\"is_payment\", (f.col(\"pagePath\") == \"/payment.html\"))\n",
    "    .withColumn(\"is_order_completed\", (f.col(\"pagePath\") == \"/ordercompleted.html\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the following columns:\n",
    "- `home_seen`: boolean column that is true if the hit happened after a hit on the home page.\n",
    "- `items_after_home`: boolean column that is true if the hit is on an item page and the home page has been seen.\n",
    "- `basket_after_item`: boolean column that is true if the hit is on the shopping cart page and there was a **previous hit on an item page, which, in turn, had a previous hit on the home page**.\n",
    "- `payment_after_basket`: boolean column that is true if the hit is on the payment page and there was a previous hit on the home, item and basket pages in sequence\n",
    "- `order_completed_after_payment`: boolean column that indicates if the hit is on the order completed page and there was a previous hit on the home, item, basket and payment pages in sequence\n",
    "\n",
    "Hint:\n",
    "- You should create them one after the other. And you should use the column you created in the previous step to create the new column.\n",
    "- To create some of these columns, you should use a window that goes from the current hit to the first hit of the session. \n",
    "- The function to be applied over the window needs to be a window function that returns True if some condition verifies. The condition is that there was a previous hit on the page you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"sessionId\").orderBy(\"hitNumber\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df = (\n",
    "    df_pages_flag\n",
    "    .withColumn(\"home_seen\", f.max(\"is_home\").over(window))\n",
    "    .withColumn(\"item_after_home\", f.col('is_item') & f.col('home_seen'))\n",
    "    .withColumn(\"basket_after_item\", f.col('is_basket') & f.max('item_after_home').over(window))\n",
    "    .withColumn(\"payment_after_basket\",\n",
    "                f.col(\"is_payment\") & f.max('basket_after_item').over(window)\n",
    "    )\n",
    "    .withColumn(\"order_completed_after_payment\",\n",
    "                f.col(\"is_order_completed\") & f.max('payment_after_basket').over(window)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculate the number of distinct sessions where each of these columns is true. This will give you the number of sessions that stop at each stage.\n",
    "\n",
    "You can store each of these values in a column of a final results dataframe.\n",
    "\n",
    "After that, calculate the percentage of sessions that reach each stage. For that, divide the number of sessions that reach a stage by the number of sessions that reached the previous stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    df\n",
    "    .agg(\n",
    "        f.countDistinct(f.when(f.col(\"home_seen\"), f.col(\"sessionId\"))).alias(\"total_home\"),\n",
    "        f.countDistinct(f.when(f.col(\"item_after_home\"), f.col(\"sessionId\"))).alias(\"total_item_after_home\"),\n",
    "        f.countDistinct(f.when(f.col(\"basket_after_item\"), f.col(\"sessionId\"))).alias(\"total_basket_after_item\"),\n",
    "        f.countDistinct(f.when(f.col(\"payment_after_basket\"), f.col(\"sessionId\"))).alias(\"total_payment_after_basket\"),\n",
    "        f.countDistinct(f.when(f.col(\"order_completed_after_payment\"), f.col(\"sessionId\"))).alias(\"total_order_completed_after_payment\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"item_after_home_ratio\",  f.col(\"total_item_after_home\") / f.col(\"total_home\")\n",
    "    ).withColumn(\n",
    "        \"basket_after_item_ratio\",  f.col(\"total_basket_after_item\") / f.col(\"total_item_after_home\")\n",
    "    ).withColumn(\n",
    "        \"payment_after_basket_ratio\", f.col(\"total_payment_after_basket\") / f.col(\"total_basket_after_item\")\n",
    "    ).withColumn(\n",
    "        \"order_completed_after_payment_ratio\", \n",
    "        f.col(\"total_order_completed_after_payment\") / f.col(\"total_payment_after_basket\")\n",
    "    )\n",
    ")\n",
    "\n",
    "result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The UDFs way\n",
    "\n",
    "Now, let's do it using UDFs.\n",
    "\n",
    "Start by creating a function returns True if there is a hit on an item page after there was a hit on the home page for each session. Then register this function as a UDF. After that, create a new boolean column on the hits dataframe that is the result of applying this UDF to the `hits` column.\n",
    "\n",
    "It is easier to use UDFs on top of the original hits dataframe rather than the exploded one. Since we have one row for each session, and each row has an array of hits, we can iterate through an array using python, and it becomes easy to deal with this complex column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "@f.udf(BooleanType())\n",
    "def item_after_home(hits):\n",
    "    # Hits is a list of dictionaries\n",
    "    visited_home = False\n",
    "    \n",
    "    for hit in hits:\n",
    "        page_path = hit['page']['pagePathLevel1']\n",
    "        if page_path == '/home':\n",
    "            visited_home = True\n",
    "        \n",
    "        if visited_home and page_path == '/google+redesign/':\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "df_hits_item_after_home = (\n",
    "    df_hits\n",
    "    .withColumn('item_after_home', item_after_home(f.col('hits')))\n",
    ")\n",
    "\n",
    "df_hits_item_after_home.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now elaborate on that function. Instead of returning just a boolean, return an array of booleans, one for each page path. Then, register this function as a UDF and apply it to the hits dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, ArrayType\n",
    "\n",
    "@f.udf(ArrayType(BooleanType()))\n",
    "def stages(hits):\n",
    "    result = []\n",
    "    \n",
    "    home = False\n",
    "    item_after_home = False\n",
    "    basket_after_item = False\n",
    "    payment_after_basket = False\n",
    "    order_completed_after_payment = False\n",
    "    \n",
    "    for hit in hits:\n",
    "        page_path = hit['page']['pagePathLevel1']\n",
    "\n",
    "        if page_path == '/home':\n",
    "            home = True\n",
    "        \n",
    "        if home and (page_path == '/google+redesign/'):\n",
    "            item_after_home = True\n",
    "        \n",
    "        if item_after_home and (page_path == '/basket.html'):\n",
    "            basket_after_item = True\n",
    "        \n",
    "        if basket_after_item and (page_path == '/payment.html'):\n",
    "            payment_after_basket = True\n",
    "        \n",
    "        if payment_after_basket and page_path == '/ordercompleted.html':\n",
    "            order_completed_after_payment = True \n",
    "\n",
    "    # Store results in a map\n",
    "    result = [\n",
    "        home,\n",
    "        item_after_home,\n",
    "        basket_after_item,\n",
    "        payment_after_basket,\n",
    "        order_completed_after_payment\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "df_stages = (\n",
    "    df_hits\n",
    "    .withColumn('stages', stages(f.col('hits')))\n",
    ")\n",
    "\n",
    "df_stages.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a boolean column for each stage. Basically, create one column for each array element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df_stages\n",
    "    .select(\n",
    "        'sessionId',\n",
    "        f.element_at('stages', 1).alias('home_seen'),\n",
    "        f.element_at('stages', 2).alias('item_after_home'),\n",
    "        f.element_at('stages', 3).alias('basket_after_item'),\n",
    "        f.element_at('stages', 4).alias('payment_after_basket'),\n",
    "        f.element_at('stages', 5).alias('order_completed_after_payment'),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now perform the same statistical calculations that you did when using the PySpark built-in functions.\n",
    "\n",
    "Using a UDF is just an alternative way of finding out if a session reached a certain stage. After this information is available, you can use the built-in functions to calculate the number of sessions that reached each stage and the percentage of sessions that reached each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    df_stages\n",
    "    .agg(\n",
    "        f.countDistinct(f.when(f.col(\"home_seen\"), f.col(\"sessionId\"))).alias(\"total_home\"),\n",
    "        f.countDistinct(f.when(f.col(\"item_after_home\"), f.col(\"sessionId\"))).alias(\"total_item_after_home\"),\n",
    "        f.countDistinct(f.when(f.col(\"basket_after_item\"), f.col(\"sessionId\"))).alias(\"total_basket_after_item\"),\n",
    "        f.countDistinct(f.when(f.col(\"payment_after_basket\"), f.col(\"sessionId\"))).alias(\"total_payment_after_basket\"),\n",
    "        f.countDistinct(f.when(f.col(\"order_completed_after_payment\"), f.col(\"sessionId\"))).alias(\"total_order_completed_after_payment\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"item_after_home_ratio\",  f.col(\"total_item_after_home\") / f.col(\"total_home\")\n",
    "    ).withColumn(\n",
    "        \"basket_after_item_ratio\",  f.col(\"total_basket_after_item\") / f.col(\"total_item_after_home\")\n",
    "    ).withColumn(\n",
    "        \"payment_after_basket_ratio\", f.col(\"total_payment_after_basket\") / f.col(\"total_basket_after_item\")\n",
    "    ).withColumn(\n",
    "        \"order_completed_after_payment_ratio\", \n",
    "        f.col(\"total_order_completed_after_payment\") / f.col(\"total_payment_after_basket\")\n",
    "    )\n",
    ")\n",
    "\n",
    "result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promotion effectiveness - More complex path analysis\n",
    "\n",
    "\n",
    "In question 6 we saw which was the most clicked promotion in sessions where products were added to the cart and the promotion was clicked.\n",
    "\n",
    "However, we are not sure if clicking the promotion is what lead to the products being added to the cart. \n",
    "\n",
    "So now let's see which promotion actually led to additions to the cart. For that we need to do sequential analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, StringType, IntegerType\n",
    "\n",
    "def promotion_purchases(hits):\n",
    "    # hits is an array of dicts\n",
    "\n",
    "    promos = {}\n",
    "    current_promo = None\n",
    "    prev_page = None\n",
    "    last_was_promo = False\n",
    "\n",
    "    for hit in hits:\n",
    "        current_page = hit['page']['pageTitle']\n",
    "\n",
    "        # If last event was promo click is first time in page.\n",
    "        # No need to compare current to last page\n",
    "        if last_was_promo:\n",
    "            last_was_promo = False\n",
    "        else:\n",
    "            # Reset current promo if the page changes\n",
    "            if current_page != prev_page:\n",
    "                current_promo = None\n",
    "\n",
    "        # Check if the event is a promotion click and update the current promo\n",
    "        if hit['eventInfo'] and (hit['eventInfo']['eventAction'] == 'Promotion Click'):\n",
    "            current_promo = hit['promotion'][0]['promoId']\n",
    "            last_was_promo = True\n",
    "\n",
    "         # If the event is 'Add to Cart' and a promo is active, update the count\n",
    "        if current_promo and hit['eventInfo'] and (hit['eventInfo']['eventAction'] == 'Add to Cart'):\n",
    "            if current_promo not in promos:\n",
    "                promos[current_promo] = 0\n",
    "            promos[current_promo] += 1\n",
    "        \n",
    "        prev_page = current_page\n",
    "    \n",
    "    return promos\n",
    "\n",
    "\n",
    "promotion_purchases_udf = f.udf(promotion_purchases, MapType(StringType(), IntegerType()))\n",
    "\n",
    "res = (\n",
    "    df_hits\n",
    "    .withColumn(\n",
    "        'promotion_purchases',\n",
    "        promotion_purchases_udf(f.col('hits'))\n",
    "    )\n",
    ")\n",
    "\n",
    "res.display()\n",
    "\n",
    "(\n",
    "    res\n",
    "    .select(\n",
    "        f.explode('promotion_purchases')\n",
    "    )\n",
    "    .withColumnRenamed('key', 'promoId')\n",
    "    .withColumnRenamed('value', 'nr_purchases')\n",
    "    .groupby('promoId')\n",
    "    .agg(f.sum('nr_purchases').alias('total_purchases'))\n",
    "    .orderBy(f.desc('total_purchases'))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get origin promo\n",
    "# if we got to a page because a promo was clicked, set origin promo to promo name\n",
    "# everytime a promo is clicked, the next hit is a 'PAGE' hit with the resulting page\n",
    "\n",
    "window_origin_promo = Window.partitionBy('visitId', 'visitNumber', 'visitorId', 'visitStartTime').orderBy('hitNumber')\n",
    "\n",
    "df_origin_promo = (\n",
    "    df_hits\n",
    "    .select(\n",
    "        'visitId',\n",
    "        'visitNumber',\n",
    "        'visitorId',\n",
    "        'visitStartTime',\n",
    "        f.inline('hits')\n",
    "    )\n",
    "    .withColumn('origin_promo',\n",
    "                f.when(\n",
    "                    f.lag(f.col('eventInfo')).over(window_origin_promo).isNotNull()\n",
    "                    & (f.lag(f.col('eventInfo').getField('eventAction')).over(window_origin_promo) == 'Promotion Click'),\n",
    "                    f.lag('promotion').over(window_origin_promo)\n",
    "                ).otherwise(None)\n",
    "    )\n",
    "    .select(\n",
    "        'visitId',\n",
    "        'visitNumber',\n",
    "        'visitorId',\n",
    "        'visitStartTime',\n",
    "        'type',\n",
    "        'hitNumber',\n",
    "        f.col('page').getField('pageTitle').alias('pageTitle'),\n",
    "        f.col('eventInfo').getField('eventAction').alias('eventAction'),\n",
    "        'promotion',\n",
    "        'origin_promo'\n",
    "    )\n",
    ")\n",
    "\n",
    "# df_origin_promo.display()\n",
    "\n",
    "\n",
    "# Now let's see what happens in each session and each visited page\n",
    "# We can only conclude that an item was added to the cart because of a promotion if a promotion click led to a page and the user left that page until adding an item to the cart\n",
    "# But how can we see if the user never left the page between a promotion click and adding an item to the cart? We need to check if the hit numbers are sequential between these two events\n",
    "\n",
    "# First create a new column with the last hit number on each page on a session\n",
    "# Then create a 'sequential' column taht is True if the row's hitNumber is equal to the last hit Number and False otherwise\n",
    "\n",
    "# Let 'sequential' be False for every time the user enters the page during the session. So, it will be false for the first time ever the user entered the page during the session bue also for every time the user entered the page, then got out and then got in again.\n",
    "\n",
    "# It is important to track this events because we will only want to consider that an addition to the cart was due to a promotion click if the user never left the page between clicking the promotion and adding the item to the cart.\n",
    "# Imagine the scenario where user clicks promotion -> gets into page A -> exists page A and visits page B -> goes back to page A -> adds item to cart\n",
    "# This case should not count for the promotion's success cases\n",
    "\n",
    "window_page = Window.partitionBy('visitId', 'visitNumber', 'visitorId', 'visitStartTime', 'pageTitle').orderBy('hitNumber')\n",
    "\n",
    "df_sequential = (\n",
    "    df_origin_promo\n",
    "    .withColumn('lastHitNumber', f.lag('hitNumber').over(window_page))\n",
    "    .withColumn('sequential', f.when(f.col('hitNumber') == (f.col('lastHitNumber') + 1), True).otherwise(False))\n",
    ")\n",
    "\n",
    "# df_sequential.display()\n",
    "\n",
    "# Now let's register the page view number. It is a column that indicates which time the page was visited in the session is that hit originated from\n",
    "# page View could be sequential but it doesn't really matter. to make things easier let's save the page view as the hit number of the hit that originated entering the page after being in a different page\n",
    "\n",
    "# Let's set the page view as the hit number and then set as the last hit number for all the rows where sequential = True\n",
    "# Need to use ignoreNulls = True\n",
    "\n",
    "df_page_view = (\n",
    "    df_sequential\n",
    "    .withColumn(\n",
    "        'pageView',\n",
    "        f.when(\n",
    "            f.col('sequential') == False,\n",
    "            f.col('hitNumber')\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    .withColumn(\n",
    "        'pageView',\n",
    "        f.last('pageView', ignorenulls=True ).over(window_page)\n",
    "    )\n",
    ")\n",
    "\n",
    "# df_page_view.display()\n",
    "\n",
    "# Finally, we can update the origin promo column to propagate to all hits on a page that followed a visist to a page that was originated from a promotion click\n",
    "\n",
    "# For that we can get the last origin promo for each page view in a session\n",
    "window_page_view = Window.partitionBy('visitId', 'visitNumber', 'visitorId', 'visitStartTime', 'pageTitle', 'pageView').orderBy('hitNumber')\n",
    "\n",
    "df_final = (\n",
    "    df_page_view\n",
    "    .withColumn(\n",
    "        'origin_promo',\n",
    "        f.last('origin_promo', ignorenulls=True).over(window_page_view)\n",
    "    )\n",
    ")\n",
    "\n",
    "# df_final.display()\n",
    "\n",
    "(\n",
    "    df_final\n",
    "    .filter(\n",
    "        (f.col('eventAction') == 'Add to Cart')\n",
    "        & (f.col('origin_promo').isNotNull())\n",
    "    )\n",
    "    .groupBy(f.element_at(f.col('origin_promo'), 1).getField('promoId').alias('promoId'))\n",
    "    .agg(f.count('visitId').alias('nr_purchases'))\n",
    "    .orderBy(f.desc('nr_purchases'))\n",
    ").display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daredata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
